{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Following - Data Collection (Mouse)\n",
    "\n",
    "If you've run through the collision avoidance sample in Day 2, you should now be familiar with these four steps:\n",
    "\n",
    "1. Data collection\n",
    "2. Training\n",
    "3. Optimizing\n",
    "3. Deployment\n",
    "\n",
    "In this notebook, we'll do the exact same thing! Except, instead of classification, you'll learn a different fundamental technique known as **regression**, which we'll use to enable our Jetbot to follow a path on the road.\n",
    "\n",
    "This notebook is for **Data Collection** only and we will be performing the following tasks:\n",
    "\n",
    "1. Display the live camera feed using a special ipywidget called jupyter clickable image widget\n",
    "2. Then collect data by performing the following steps:\n",
    "- Place the JetBot at different positions on a path (offset from center, different angles, etc)\n",
    "- Using your mouse, place a 'green dot', which corresponds to the target direction we want the robot to travel on.\n",
    "- Store the X, Y values of this green dot along with the image from the robot's camera\n",
    "\n",
    "What we get, is a 'carrot on a stick' that moves along our desired trajectory.  Deep learning decides where to place the carrot, and JetBot just follows it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies!\n",
    "By default `jupyter_clickable_image_widget` is not installed on the Jetbot therefore we would need to first install it before continuing on with this notebook. To do that, go to your terminal and paste the following commands line by line:   \n",
    "1) cd   \n",
    "2) sudo apt-get install nodejs-dev node-gyp libssl1.0-dev   \n",
    "3) sudo apt-get install npm    \n",
    "4) git clone https://github.com/jaybdub/jupyter_clickable_image_widget   \n",
    "5) cd jupyter_clickable_image_widget   \n",
    "6) git checkout no_typescript   \n",
    "7) sudo pip3 install -e .   \n",
    "8) sudo jupyter labextension install js   \n",
    "9) sudo jupyter lab build   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "So lets get started by importing all the required libraries for \"data collection\" purpose. We will mainly use OpenCV to visualize and save image with labels. Libraries such as uuid, datetime are used for image naming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython Libraries for display and widgets\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Camera and Motor Interface for JetBot\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "\n",
    "# Basic python packages for image annotation\n",
    "from uuid import uuid1\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data\n",
    "\n",
    "Firstly, let's initialize and display our camera like we did in the previous notebooks, however, this time with using a special ipywidget called `jupyter_clickable_image_widget` that lets you click on the image and take the coordinates for data annotation. This eliminates the needs of using the gamepad controller for data annotation.\n",
    "\n",
    "Our neural network takes a 224x224 pixel image as input. We'll set our camera to that size to minimize the filesize of our dataset. In some scenarios it may be better to collect data in a larger image size and downscale to the desired size later.\n",
    "\n",
    "The following block of code below will display the live image feed for you to click on for annotation on the left, as well as the snapshot of last annotated image (with a green circle showing where you clicked) on the right.     \n",
    "\n",
    "When you click on the left live image, it stores a file in the ``dataset_xy`` folder with files named\n",
    "\n",
    "``xy_<x value>_<y value>_<uuid>.jpg``\n",
    "\n",
    "When we train, we load the images and parse the x, y values from the filename.\n",
    "Here `<x value>` and `<y value>` are the coordinates **in pixels** (counted from the top left corner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_clickable_image_widget import ClickableImageWidget\n",
    "\n",
    "DATASET_DIR = 'dataset_xy'\n",
    "\n",
    "# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n",
    "try:\n",
    "    os.makedirs(DATASET_DIR)\n",
    "except FileExistsError:\n",
    "    print('Directories not created because they already exist')\n",
    "\n",
    "camera = Camera()\n",
    "\n",
    "# create image preview\n",
    "camera_widget = ClickableImageWidget(width=camera.width, height=camera.height)\n",
    "snapshot_widget = ipywidgets.Image(width=camera.width, height=camera.height)\n",
    "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# create widgets\n",
    "count_widget = ipywidgets.IntText(description='count')\n",
    "# manually update counts at initialization\n",
    "count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "\n",
    "def save_snapshot(_, content, msg):\n",
    "    if content['event'] == 'click':\n",
    "        data = content['eventData']\n",
    "        x = data['offsetX']\n",
    "        y = data['offsetY']\n",
    "        \n",
    "        # save to disk\n",
    "        uuid = 'xy_%03d_%03d_%s' % (x, y, uuid1())\n",
    "        image_path = os.path.join(DATASET_DIR, uuid + '.jpg')\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(camera_widget.value)\n",
    "        \n",
    "        # display saved snapshot\n",
    "        snapshot = camera.value.copy()\n",
    "        snapshot = cv2.circle(snapshot, (x, y), 8, (0, 255, 0), 3)\n",
    "        snapshot_widget.value = bgr8_to_jpeg(snapshot)\n",
    "        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "        \n",
    "camera_widget.on_msg(save_snapshot)\n",
    "\n",
    "data_collection_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([camera_widget, snapshot_widget]),\n",
    "    count_widget\n",
    "])\n",
    "\n",
    "display(data_collection_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's close the camera connection properly so that we can use the camera in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset in a zip (optional)\n",
    "\n",
    "Once you've collected enough data, we can choose to copy that data to a GPU desktop or cloud machine for training. To do that, we can call the following *terminal* command to compress\n",
    "our dataset folder into a single *zip* file.\n",
    "\n",
    "> The ! prefix indicates that we want to run the cell as a *shell* (or *terminal*) command.\n",
    "\n",
    "> The -r flag in the zip command below indicates *recursive* so that we include all nested files, the -q flag indicates *quiet* so that the zip command doesn't print any output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r -q dataset_mouse.zip dataset_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a file named ``dataset_mouse.zip`` appear in the Jupyter Lab file browser.\n",
    "\n",
    "Next, Let's move on to the \"train_model.ipynb\" notebook so that we can start training our model on the collected dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
